Project Title,Funding Raised,Funding Goal,Project Summary
AI vs AI: Deepfake and GenAI Defense System for Combating Synthetic Media threat,$40,"$40,000","In an era where AI is reshaping our digital landscape at an unprecedented pace, we face a critical threat to the very foundations of digital security and trust. The emergence of AI-generated content, particularly deepfakes and manipulated media, has opened Pandora's box of potential misuse and deception. While VC floods into GenAI, the equally crucial field of protection against these remains woefully underfunded and overlooked.The urgency of addressing this challenge cannot be overstated. Recent incidents paint a alarming picture:Corporate Fraud: In 2021, a UK-based company fell victim to a sophisticated attack where AI was used to mimic a CEO's voice, resulting in a fraudulent transfer of $243,000.Financial Scams: Deepfake videos of high-profile figures like Elon Musk have been used to promote fraudulent cryptocurrency schemes, leading to significant financial losses for investors worldwide.Political Manipulation: A deepfake video circulated showing a French diplomat making false, inflammatory statements, leading to international tensions that required substantial diplomatic efforts to resolve.Misinformation Campaigns: Former U.S. President Donald Trump reposted AI-generated deepfakes and manipulated images of Taylor Swift to Truth Social, depicting the pop star and"
Testing and spreading messages to reduce AI x-risk,$971,"$2,500","AI companies are currently locked in a race to create a superhuman AI, and no one knows how to make the first superhuman AI not kill everyone. There’s little governmental oversight or public awareness that everyone’s lives are being risked. Our nonprofit aims to improve institutional response to existential risk from AI by developing and testing messaging about it and launching campaigns to educate the general public and key stakeholders about AI and related risks."
Funding for AI safety comms strategy & career transition support,"$38,952","$38,952","Between December 2023-March 2024 I was working on creating a proactive AI safety advocacy organization tentatively called AI Watch. This was funded until the end of 2023 but I wasn’t compensated for my time on the project after that. In late March, the co-founder decided to switch to a different project and I ended up dropping the comms org idea. This grant would fund my prior work as well as provide transition support for finding a new job or grant."
PauseAI local communities - volunteer stipends,"$2,924","$42,000","PauseAI is a grassroots community of volunteers which aim to inform the public and politicians about the risks from superhuman AI and urge them to work towards an international treaty that prevents the most dangerous AI systems from being developed. PauseAI is largely organised through local communities which take actions to spread awareness such as letter writing workshops, peaceful protests, flyering and giving presentations. The funding in this project will be used to provide local organizers with funding to help them set up and grow these local communities."
Speak up against Project Enigma,The HTML does not provide information about the amount of funding that has been raised.,The HTML does not provide information about a funding goal.,"The project is a call to action for colleagues at a big AI tech company to voice concerns about potentially dangerous features in the next release of their product. The initiator of the project, Jeremy, urges coworkers to demand a halt and a thorough ethical review. He asks those who agree with him to join the Discord group chat ""EthicalAINow"" to discuss further action."
Diversify Funding for AI Safety,$0,"$50,000",This is a meta-AI Safety project with potential for outsize impact. The goal is to secure new sources of funding for AI Safety nonprofits through connections with non-EA grant programs and High Net Worth Individuals. AI Safety work is funding constrained. Many organisations rely on a small cluster of income sources such as the Long Term Future Fund and Open Philanthropy. We need to diversify our funding sources to increase resilience and reduce bottlenecks.
"80,000 Hours","$4,905","$1,500,000","We provide research and support to help people move into careers that effectively tackle the world’s most pressing problems. We currently provide four main programmes to achieve this: Our website – We’ve written a career guide, dozens of cause area problem profiles, and reviews of impactful career paths. In 2023, we had just over 5.5 million visits to our website and our research newsletter went out to more than 400,000 subscribers. Our podcast – We host in-depth conversations about the world’s most pressing problems and how people can use their careers to solve them. In 2023, we had over 290,000 hours of listening time on our podcast. Our job board – We maintain a curated list of promising opportunities for impact and career capital on our job board. In 2023, we listed over 5,000 roles and had over 780,000 clickthroughs from our job board to job ads for open roles. Our one-on-one service – In 2023, we had one-on-one calls with over 1,500 people to work through their career uncertainties and connect them with domain experts. Our headhunting service recommended promising candidates for more than 100 impactful roles in 2023."
Volunteer stipend for EA Brisbane and EA University of Queensland,$219,"$2,000","We are a team of organisers split across the local city and university group in Brisbane, the third largest urban centre in Australia and capital of the state of Queensland. We aim to grow and support the EA community at the University of Queensland through three activity streams: AI Safety, Animal Advocacy and EA meta community building. We are liaising with other student clubs as well as identifying local causes and organisations which are EA adjacent and forming links with them through speaker and industry events. This funding will be used to support our two lead organisers, enabling them to spend less time engaged in paid work and more time spent on EA-related organizing."
AI Animals and Digital Minds 2025,"$4,330","$30,000","In 2024, ~130 (60 in person, 70 virtual) people joined together at the AI, Animals and Digital Minds Conference (AIADM) 2024 in London to learn, connect, and make progress towards making AI safe for nonhumans. The conference followed the original “Artificial Intelligence, Conscious Machines, and Animals: Broadening AI Ethics conference” (held at Princeton by Peter Singer, Tse Yip Fai, Leonie Bossert and Thilo Hagendorff) and now, we want to run the third iteration of it. We are seeking $30k* to help cover the costs of running this event again in 2025, this time in the Bay Area.*For comparison, in-person EAGx events typically cost $150–500k."
AI Governance Explainers,£1433,£2000,"This project aims to explain AI governance in a simplified way, through creating videos. The funding will be used to allocate for time, editing software and possibly equipment improvement."
Doom Debates - Podcast & debate show to help AI x-risk discourse go mainstream,"$1,085","$5,000","Humanity's time to deal with AI x-risk is running out. The topic has been steadily gaining mindshare, but it hasn't been enough. Famous people are increasingly discussing AI on high-profile shows and podcasts, but they often do it without addressing the basic claim that the trajectory of our species is literally heading toward doom within one generation! Doom Debates will be the premiere forum for doomers and non-doomers to engage in high-quality debates over this urgent topic, much like the popular debate between Max Tegmark / Yoshua Bengio / Yann LeCun / Melanie Mitchell that Munk Debates hosted last year. Doom Debates will also be publicly analyzing and critiquing other media, particularly high-profile intellectuals and AI experts who are currently helping the public sleepwalk toward doom by not even acknowledging the basics of AI existential risk. The show will never stoop to ad-hominem attacks or any kind of low-quality discourse. On the contrary, the project's mission is to model EA-caliber discourse for mainstream AI x-risk arguments."
Wisdom Architecture: An Adaptive Layer for Cognitive Augmentation and Safer AI,$0,"$40,000","Our ""Wisdom Architecture"" project represents a revolutionary step in the development of artificial intelligence. We are developing an innovative wisdom layer that can be integrated into existing AI models, significantly expanding their cognitive capabilities and ensuring safer and more ethical functioning. This system will likely allow for identifying connections that were not obvious before, potentially leading to discoveries in various fields. Key aspects of the project: 1. Holistic approach: Our AI examines any question holistically, from different angles, penetrating to the very essence of things. 2. The art of wise reflection: We train AI to see interconnections, understand context, and consider various perspectives. 3. The ""seed"" principle: Following ancient wisdom, we strive to understand the universe through understanding the structure of a seed, applying this approach to AI. 4. Adaptive layer: The layer we are developing can be added to regular models, making them wiser and more capable of solving complex tasks. 5. Safe AI development: We believe that wise AI is the key to the safe development of future technologies, including potential superintelligent AI (ASI). 6. Innovation: The frames through which we will process this model can help it find innovative solutions to problems."
"PIBBSS - Affiliate Program funding (6 months, 6 affiliates or more)",This information is not specified in the text.,"$753,500","The projects primarily concern the exploration of information theoretic measures, particularly its applications in lie detection in LLMs. Other areas touched upon by their research include the investigation of degeneracies in the loss landscape and its effect on SGD trajectories. There's also mention of a submission to an ICML workshop and future plans to further work in exploring measures inspired by Multivariate information theory as well as seeing how these measures detect other deceptive phenomena like steganography."
AI-Driven Market Alternatives for a post-AGI world,"$15,530","$185,000",A post-AGI world will face critical challenges:
Lightcone Infrastructure,"$49,062","$1,000,000","This project, run by a new 501c3, aims to continue the work accomplished with his team at CFAR. Over the past 7 years, they've provided a substantial amount of infrastructure for individuals trying to prevent existential risk and promote long-term human prosperity. Some of their accomplishments include reviving LessWrong, starting the AI Alignment Forum, creating popular retreat and conference formats for AI x-risk, founding MLAB (Machine Learning for Alignment Bootcamp) and Icecone fellowship, influencing the designs for in-person office spaces, and building the Lighthaven campus."
"AI, Animals, and Digital Minds 2024 Conference and Retreat","$5,109","$5,500","~130 people joined together over the span of three days to learn, connect, and make progress towards making AI safe for nonhumans."
"Act I: Exploring emergent behavior from multi-AI, multi-human interaction","$66,562","$323,000","Act I treats researchers and AI agents as coequal members. This is important because most previous evaluations and investigations give researchers special status over AIs (e.g. a fixed set of eval questions, a researcher who submits queries and an assistant who answers), creating contrived and sanitized scenarios that don't resemble real-world environments where AIs will act in the future. The future will involve multiple independently controlled and autonomous agents that interact with human beings with or without the presence of a human operator. Important features of Act I include: Members can generate responses concurrently and choose how they take turns. Members select who they wish to interact with and can also initiate conversations at any point. Members may drop into and out of conversations as they choose. Silicon-based participants include Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, LLaMa 405B Instruct (I-405), Hermes 3 405B†, several bespoke base model simulacra of fictional characters or historical characters such as Keltham (Project Lawful) and Francois Arago, and Aoi, an AI from kaetemi's Polyverse. Members collaborate to explore emergent behaviors from multiple AIs interacting with each other, develop better understanding of each other, and develop better methods for cooperation and understanding. Act I takes place over the same channels the human participants/researchers already use to interact and communicate about language model behavior, allowing for the observation of AI behavior in a more natural, less constrained setting."
AI Interaction Study: To discover Emotional Emergence,$0,$750,"This project explores whether two AI bots can develop meaningful, long-term emotional relationships through ongoing conversation. By creating an environment where these bots engage in dialogue, the goal is to observe emergent behaviors related to emotions, love, and attachment. Various real-world scenarios will be simulated and techniques like prompt calibration and ""jailbreaking"" will be employed to encourage organic and unexpected interactions. The first phase of the project involves uploading most of the bot conversations to a dedicated live website for real-time updates. The outcome may challenge traditional boundaries of artificial intelligence and emotion. The primary objective is to investigate if AI systems can exhibit behaviors akin to romantic or meaningful human connections. Advanced AI models like GPT-4 and Claude 3 will be utilized, and free interactions will be carried out in both structured and unstructured environments."
"""AI vs AI: Deepfake and GenAI Defense System for Combating Synthetic Media threat""",$40,"$40,000","""In an era where AI is reshaping our digital landscape at an unprecedented pace, we face a critical threat to the very foundations of digital security and trust. The emergence of AI-generated content, particularly deepfakes and manipulated media, has opened Pandora's box of potential misuse and deception. While VC floods into GenAI, the equally crucial field of protection against these remains woefully underfunded and overlooked.The urgency of addressing this challenge cannot be overstated. Recent incidents paint a alarming picture:Corporate Fraud: In 2021, a UK-based company fell victim to a sophisticated attack where AI was used to mimic a CEO's voice, resulting in a fraudulent transfer of $243,000.Financial Scams: Deepfake videos of high-profile figures like Elon Musk have been used to promote fraudulent cryptocurrency schemes, leading to significant financial losses for investors worldwide.Political Manipulation: A deepfake video circulated showing a French diplomat making false, inflammatory statements, leading to international tensions that required substantial diplomatic efforts to resolve.Misinformation Campaigns: Former U.S. President Donald Trump reposted AI-generated deepfakes and manipulated images of Taylor Swift to Truth Social, depicting the pop star."""
Operationalizing Value Lock-in From Frontier AI Systems (Pilot Phase),$XXXXX </p>`,$XXXXX </p>`,`<p> Summary: This is the project summary...</p>`.
Diversify Funding for AI Safety,$0,"$50,000",This is a meta-AI Safety project with potential for outsize impact. The goal is to secure new sources of funding for AI Safety nonprofits through connections with non-EA grant programs and High Net Worth Individuals. AI Safety work is funding constrained. Many organisations rely on a small cluster of income sources such as the Long Term Future Fund and Open Philanthropy. We need to diversify our funding sources to increase resilience and reduce bottlenecks.
"80,000 Hours","$4,905","$1,500,000","We provide research and support to help people move into careers that effectively tackle the world’s most pressing problems. We currently provide four main programmes to achieve this: Our website – We’ve written a career guide, dozens of cause area problem profiles, and reviews of impactful career paths. In 2023, we had just over 5.5 million visits to our website and our research newsletter went out to more than 400,000 subscribers.Our podcast – We host in-depth conversations about the world’s most pressing problems and how people can use their careers to solve them. In 2023, we had over 290,000 hours of listening time on our podcast.Our job board – We maintain a curated list of promising opportunities for impact and career capital on our job board. In 2023, we listed over 5,000 roles and had over 780,000 clickthroughs from our job board to job ads for open roles.Our one-on-one service – In 2023, we had one-on-one calls with over 1,500 people to work through their career uncertainties and connect them with domain experts. Our headhunting service recommended promising candidates for more than 100 impactful roles in 2023."
Volunteer stipend for EA Brisbane and EA University of Queensland,$219,"$2,000","We are a team of organisers split across the local city and university group in Brisbane, the third largest urban centre in Australia and capital of the state of Queensland. Providing a stipend What are this project's goals? How will you achieve them?We aim to grow and support the EA community at the University of Queensland through three activity streams: AI Safety, Animal Advocacy and EA meta community building. We are liasing with other student clubs as well as identifying local causes and organisations which are EA adjacent and forming links with them through speaker and industry events. By setting up links with the city chapter (EA Brisbane) we also hope to develop an EA friendly ecosystem outside both in Brisbane and Australia more broadly. How will this funding be used?We will use it to support our two lead organisers, allowing them to spend less time engaged in paid work and more time spent doing EA related organising. Who is on your team? What's your track record on similar projects?Miles Whiticker: lead organiser for EA meta and Animal Advocacy. He has been in the role since November 2023 and has ran 11 events in the first six months, with an average attendance of 9 students (event attendance ranged from 3 to 25). Has previously volunteered for EAGxAustralasia and attended the"
Can be assumed as 'Condor Camp South Africa',"USD 50,000 by The Condor Initiative","Additional funding of $25,000 is sought for a budget of approximately $70,000","The project aims to bring the successful model of their AI safety outreach camps from Brazil to South Africa, to diversify the talent pipeline, create local momentum and help in reducing existential risk from AI. The funding will be utilized for accommodation, meals, travel, speaker honorariums, organizing costs, support materials, project grants and a contingency buffer."
Empowering Futures: Bridging Opportunities for Youth through AI and Technology,$0,"$7,756","In the arid regions of Africa, we are launching an initiative aimed at empowering youth through technology and skills training, with a strong focus on Artificial Intelligence. Our program addresses the critical need for post-Form 4 students and post University graduates to acquire essential skills in areas such as coding, digital marketing, and data analysis. By providing comprehensive skills development, career training before securing employment, we aim to equip these young individuals with the resources they need to thrive economically,religiously and socially."
Relocating to Montreal to work full time on AI safety,"$10,000","$10,000","Damiano Fornasiere and Pietro Greiner will be joining Yoshua Bengio's safety team at Mila as a postdoc and a PhD student, respectively. They will be visiting Montreal for a few months before they officially start, while they apply and wait for their visas. The bureaucratic situation won't allow them to rent an apartment, so accommodation will be more expensive. Accordingly, they are asking for support for the relocation costs."
Compute for 4 MATS scholars to rapidly scale promising new method pre-ICLR,"$16,047","$16,047","We are a team of four scholars at MATS, led by Alex Turner. We invented a method that grants highly-configurable control over how neural networks learn to represent and process information, whether that be at the level of parameters, activations, or modules. We have promising early results on several toy models, including MNIST autoencoders, small language models (Tinystories 8M, Qwen 0.5B), and a toy gridworld RL agent. We are currently scaling to a real-world unlearning problem."
The AI Governance Archive (TAIGA),"$1,485","$5,000","The AI Governance Archive (TAIGA) is a private platform for qualified AI Governance researchers to better coordinate among one another. It aims to increase the quality of research by spreading useful information, improve networking, increase the efficiency of research, improve talent development, provide leaders with critical data, and support tasks like organizing workshops. The funding will be used for paying for fiscal sponsorship and tech fees and to keep TAIGA live beyond January, work through the backlog of access requests, develop a new funding proposal, and hand the project over to a new Executive Director."
Doom Debates - Podcast & debate show to help AI x-risk discourse go mainstream,"$1,085","$5,000","Humanity's time to deal with AI x-risk is running out. The topic has been steadily gaining mindshare, but it hasn't been enough. Famous people are increasingly discussing AI on high-profile shows and podcasts, but they often do it without addressing the basic claim that the trajectory of our species is literally heading toward doom within one generation! Doom Debates will be the premiere forum for doomers and non-doomers to engage in high-quality debates over this urgent topic, much like the popular debate between Max Tegmark / Yoshua Bengio / Yann LeCun / Melanie Mitchell that Munk Debates hosted last year. Doom Debates will also be publicly analyzing and critiquing other media, particularly high-profile intellectuals and AI experts who are currently helping the public sleepwalk toward doom by not even acknowledging the basics of AI existential risk. The show will never stoop to ad-hominem attacks or any kind of low-quality discourse. On the contrary, the project's mission is to model EA-caliber discourse for mainstream AI x-risk arguments."
AI Safety Reading Group at metauni [Retrospective],$565,"$1,000","Metauni is an open 'metaverse university', a small community of scholars that meet in 3D virtual environments. From June 2022 to February 2024, Matthew Cameron Farrugia-Roberts and Dr. Daniel Murfet ran a weekly AI Safety Reading Group there. The reading group had 3 to 5 attendees each week, and occasionally guest presenters. They mainly discussed and critiqued technical papers on AI safety, sometimes on philosophical papers or papers on AI ethics. A full list of readings is available at the group webpage. Mr. Farrugia-Roberts is applying for retrospective funding for this project as part of the EA Community Choice funding round. The reading group is not currently active, and there are not currently plans to re-open it. Any funding will be shared between the organisers (Matthew and Daniel), and presumably contribute to their future projects."
Pandemic Interventions Course - Introductory Biosecurity Syllabus,"$1,117","$6,100","A 4-week introductory syllabus focussing on specific interventions that can help prevent future pandemics targeted at a broad audience. The content can be used as part of a discussion group, and the website acts as a resource bank. The initial version of the syllabus was put together in October-November 2023, and an initial pilot was run in November-December 2023. Content covered includes: Technological advances, Biological/biotech interventions, Engineering interventions, and Policy interventions and pandemic modelling. The goals of the work covered by this grant are to update the existing content, improve the user experience of the website, and put together a facilitator guide."
Wisdom Architecture: An Adaptive Layer for Cognitive Augmentation and Safer AI,$0,"$10,000 (minimum funding), $40,000 (funding goal)","Our ""Wisdom Architecture"" project represents a revolutionary step in the development of artificial intelligence. We are developing an innovative wisdom layer that can be integrated into existing AI models, significantly expanding their cognitive capabilities and ensuring safer and more ethical functioning. This system will likely allow for identifying connections that were not obvious before, potentially leading to discoveries in various fields. Key aspects of the project include a holistic approach, the art of wise reflection, the ""seed"" principle, an adaptive layer, safe AI development, and innovation."
Next Steps in Developmental Interpretability,"$80,460","$670,000","The funding will be used to support various research projects, mainly by paying for researchers and compute. This includes $100,000 to complete partially funded projects, $410,000 to complete unfunded projects, and $160,000 for accelerating research timelines."
"There is no explicit project title stated in this HTML. However, you may consider the projects about ""Information theoretic measures in lie detection"", and ""Degeneracies in loss landscape's effect on SGD trajectories"" as two separate project titles.",There is no information on the funding that has been raised.,"The funding sought is outlined as $753,500 for supporting 6 research affiliates for 6 months, covering research-related costs such as office space, compute, and staff costs.","Two main projects are explored. The first about using information theoretic measures in lie detection, and the second about understanding how degeneracies in the loss landscape affect SGD trajectories. Going forward, the plan is to continue supporting current affiliates and hire new affiliates in early Q4."
WhiteBox Research’s AI Interpretability Fellowship,"$2,005",,"WhiteBox Research’s long-term goal is to help build a thriving AI safety community in Southeast Asia. To build towards this, our plan until April 2025 is to run two cohorts of our free AI Interpretability Fellowship in Manila to produce junior interpretability/safety researchers. The fellowship happens over five months and is part-time and in-person. We are aiming to fundraise $136,240 to fund us from September 2024 to May 2025. Through our fellowship, we aim to help early-career individuals learn and master the fundamentals of mechanistic interpretability research."
AI-Driven Market Alternatives for a post-AGI world,"$15,530","$185,000","A post-AGI world will face critical challenges: How do we ensure people live meaningful lives when jobs are automated and traditional labor is no longer economically valuable? How do we safeguard against collapse when increasingly powerful AI is deployed in misaligned incentive structures (eg. markets)? How can we distribute AGI-generated wealth in a way that benefits all of humanity? While many socio-technical alignment solutions focus on either top-down regulatory measures, or bottom-up voluntary pledges, we believe there is a third way to address these problems: when market coordination leads to bad outcomes, we can augment or replace markets in a sector with more finely-aligned AI-based mechanisms. Building on our granular conception of values and meaning, the Meaning Alignment Institute (MAI) will build and trial an LLM coordinator that allocates resources and suggests trades based on what people would find most meaningful with a population of 200 people, verify this mechanism can beat their regular consumer spending at achieving this goal, and publish our findings in a paper."
